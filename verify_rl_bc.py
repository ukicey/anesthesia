#!/usr/bin/env python3
"""
RL+BCæ··åˆè®­ç»ƒå®ç°éªŒè¯è„šæœ¬

è¿™ä¸ªè„šæœ¬ç”¨äºéªŒè¯RL+BCå®ç°çš„æ­£ç¡®æ€§ï¼ŒåŒ…æ‹¬ï¼š
1. é…ç½®å‚æ•°æ£€æŸ¥
2. æ¨¡å‹è¾“å‡ºæ ¼å¼éªŒè¯
3. æŸå¤±è®¡ç®—éªŒè¯
"""

import torch
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

try:
    from constant import (
        use_rl_bc_hybrid, lambda_bc, lambda_rl, teacher_forcing,
        n_action, n_option, n_reward_max, n_value_max, max_lenth, 
        n_bis, n_rp, n_hidden, pre_len
    )
    from RL.Module.model import TransformerPlanningModel
    from RL.Module.pl_model import RLSLModelModule
    print("âœ… å¯¼å…¥æˆåŠŸ")
except Exception as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)


def verify_config():
    """éªŒè¯é…ç½®å‚æ•°"""
    print("\n" + "="*60)
    print("1. é…ç½®å‚æ•°éªŒè¯")
    print("="*60)
    
    print(f"use_rl_bc_hybrid: {use_rl_bc_hybrid}")
    print(f"lambda_bc: {lambda_bc}")
    print(f"lambda_rl: {lambda_rl}")
    print(f"teacher_forcing: {teacher_forcing}")
    
    # éªŒè¯æƒé‡å’Œä¸º1
    if abs((lambda_bc + lambda_rl) - 1.0) < 1e-6:
        print("âœ… BCå’ŒRLæƒé‡å’Œä¸º1.0")
    else:
        print(f"âš ï¸  BCå’ŒRLæƒé‡å’Œä¸º {lambda_bc + lambda_rl}ï¼Œä¸ç­‰äº1.0")
    
    # éªŒè¯æƒé‡èŒƒå›´
    if 0 <= lambda_bc <= 1 and 0 <= lambda_rl <= 1:
        print("âœ… æƒé‡åœ¨æœ‰æ•ˆèŒƒå›´å†… [0, 1]")
    else:
        print("âŒ æƒé‡è¶…å‡ºæœ‰æ•ˆèŒƒå›´")
        return False
    
    return True


def verify_model_output():
    """éªŒè¯æ¨¡å‹è¾“å‡ºæ ¼å¼"""
    print("\n" + "="*60)
    print("2. æ¨¡å‹è¾“å‡ºæ ¼å¼éªŒè¯")
    print("="*60)
    
    try:
        # åˆ›å»ºæ¨¡å‹
        n_aux = 3
        n_input = 381
        model = TransformerPlanningModel(
            n_action=n_action,
            n_option=n_option,
            n_reward_max=n_reward_max,
            n_value_max=n_value_max,
            max_lenth=max_lenth,
            n_aux=n_aux,
            n_input=n_input,
        )
        print("âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # åˆ›å»ºè™šæ‹Ÿè¾“å…¥
        batch_size = 2
        obs = torch.randn(batch_size, max_lenth, n_input)
        action_prev = (
            torch.randint(0, n_action, (batch_size, max_lenth + pre_len)),
            torch.randint(0, n_action, (batch_size, max_lenth + pre_len))
        )
        option_prev = torch.randint(0, n_option, (batch_size, max_lenth + pre_len))
        padding_mask = torch.zeros(batch_size, max_lenth, dtype=torch.bool)
        
        print(f"âœ… è¾“å…¥æ•°æ®åˆ›å»ºæˆåŠŸ")
        print(f"   - obs shape: {obs.shape}")
        print(f"   - action_prev shapes: {action_prev[0].shape}, {action_prev[1].shape}")
        print(f"   - option_prev shape: {option_prev.shape}")
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            outputs = model(obs, action_prev, option_prev, padding_mask, gamma=0.9)
        
        print("âœ… å‰å‘ä¼ æ’­æˆåŠŸ")
        
        # éªŒè¯è¾“å‡º
        required_keys = [
            'policy', 'value', 'reward', 'bis', 'rp',
            'policy_train', 'policy_action_logprob',
            'policy_reward', 'policy_return',
        ]
        
        if use_rl_bc_hybrid:
            required_keys.extend(['rl_action_target', 'rl_weights'])
        
        missing_keys = [k for k in required_keys if k not in outputs]
        if missing_keys:
            print(f"âŒ ç¼ºå°‘è¾“å‡ºé”®: {missing_keys}")
            return False
        
        print("âœ… æ‰€æœ‰å¿…éœ€çš„è¾“å‡ºé”®éƒ½å­˜åœ¨")
        
        # éªŒè¯RL+BCç‰¹å®šè¾“å‡º
        if use_rl_bc_hybrid:
            rl_action_target = outputs['rl_action_target']
            rl_weights = outputs['rl_weights']
            
            print(f"   - rl_action_target shapes: {rl_action_target[0].shape}, {rl_action_target[1].shape}")
            print(f"   - rl_weights shape: {rl_weights.shape}")
            
            # éªŒè¯å½¢çŠ¶
            expected_shape = (batch_size, pre_len)
            if rl_action_target[0].shape == expected_shape and rl_action_target[1].shape == expected_shape:
                print("âœ… RLç›®æ ‡åŠ¨ä½œå½¢çŠ¶æ­£ç¡®")
            else:
                print(f"âŒ RLç›®æ ‡åŠ¨ä½œå½¢çŠ¶é”™è¯¯ï¼ŒæœŸæœ› {expected_shape}")
                return False
            
            if rl_weights.shape == expected_shape:
                print("âœ… RLæƒé‡å½¢çŠ¶æ­£ç¡®")
            else:
                print(f"âŒ RLæƒé‡å½¢çŠ¶é”™è¯¯ï¼ŒæœŸæœ› {expected_shape}")
                return False
            
            # éªŒè¯æƒé‡èŒƒå›´
            if (rl_weights >= 0).all() and (rl_weights <= 1).all():
                print("âœ… RLæƒé‡åœ¨æœ‰æ•ˆèŒƒå›´ [0, 1]")
            else:
                print("âŒ RLæƒé‡è¶…å‡ºæœ‰æ•ˆèŒƒå›´")
                return False
            
            # ç»Ÿè®¡RLè§¦å‘ç‡
            rl_trigger_rate = rl_weights.mean().item()
            print(f"â„¹ï¸  RLè§¦å‘ç‡: {rl_trigger_rate*100:.2f}%")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹éªŒè¯å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def verify_loss_computation():
    """éªŒè¯æŸå¤±è®¡ç®—"""
    print("\n" + "="*60)
    print("3. æŸå¤±è®¡ç®—éªŒè¯")
    print("="*60)
    
    try:
        from RL.Module.model import TransformerPlanningModel
        from RL.Module.pl_model import RLSLModelModule
        
        # åˆ›å»ºæ¨¡å‹
        n_aux = 3
        n_input = 381
        model = TransformerPlanningModel(
            n_action=n_action,
            n_option=n_option,
            n_reward_max=n_reward_max,
            n_value_max=n_value_max,
            max_lenth=max_lenth,
            n_aux=n_aux,
            n_input=n_input,
        )
        
        # åˆ›å»ºè®­ç»ƒæ¨¡å—
        pl_model = RLSLModelModule(
            model=model,
            lr=0.001,
            model_args={},
            module_args={'loss_rl_joint': True}
        )
        
        print("âœ… è®­ç»ƒæ¨¡å—åˆ›å»ºæˆåŠŸ")
        
        # åˆ›å»ºè™šæ‹Ÿæ•°æ®
        batch_size = 2
        obs = torch.randn(batch_size, max_lenth, n_input)
        action_all = (
            torch.randint(0, n_action, (batch_size, max_lenth + pre_len)),
            torch.randint(0, n_action, (batch_size, max_lenth + pre_len))
        )
        option_all = torch.randint(0, n_option, (batch_size, max_lenth + pre_len))
        padding = torch.zeros(batch_size, max_lenth, dtype=torch.bool)
        
        # åˆ›å»ºæ ‡ç­¾æ•°æ®
        label_data = {
            'action': (
                action_all[0][:, max_lenth:max_lenth+pre_len],
                action_all[1][:, max_lenth:max_lenth+pre_len]
            ),
            'mask_reward': torch.ones(batch_size, max_lenth + pre_len),
            'bis_target': torch.randint(0, n_bis+1, (batch_size, max_lenth + pre_len)),
            'rp_target': torch.randint(0, n_rp+1, (batch_size, max_lenth + pre_len)),
            'reward': torch.randn(batch_size, max_lenth + pre_len),
            'cumreward': torch.randn(batch_size, max_lenth + pre_len),
            'padding': padding,
        }
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            pred_data = model(obs, action_all, option_all, padding, gamma=0.9)
        
        # Stack label data
        label_data_stacked, stack_mask = pl_model.label_data_stack(label_data, pre_len)
        
        # è®¡ç®—æŸå¤±
        losses = pl_model.get_loss(pred_data, label_data_stacked)
        
        print("âœ… æŸå¤±è®¡ç®—æˆåŠŸ")
        
        # éªŒè¯æŸå¤±é”®
        required_loss_keys = ['loss', 'loss_action', 'loss_bc', 'loss_rl']
        missing_loss_keys = [k for k in required_loss_keys if k not in losses]
        if missing_loss_keys:
            print(f"âŒ ç¼ºå°‘æŸå¤±é”®: {missing_loss_keys}")
            return False
        
        print("âœ… æ‰€æœ‰å¿…éœ€çš„æŸå¤±é”®éƒ½å­˜åœ¨")
        
        # æ‰“å°æŸå¤±å€¼
        print(f"\næŸå¤±å€¼:")
        print(f"   - loss_action: {losses['loss_action'].item():.4f}")
        print(f"   - loss_bc: {losses['loss_bc'].item():.4f}")
        print(f"   - loss_rl: {losses['loss_rl'].item():.4f}")
        
        # éªŒè¯æ··åˆæŸå¤±
        if use_rl_bc_hybrid:
            # éªŒè¯æŸå¤±ç»„åˆ
            expected_loss = lambda_bc * losses['loss_bc'] + lambda_rl * losses['loss_rl']
            actual_loss = losses['loss_action']
            
            # ç”±äºå¯èƒ½æœ‰æ•°å€¼è¯¯å·®ï¼Œä½¿ç”¨ç›¸å¯¹è¯¯å·®
            if torch.allclose(expected_loss, actual_loss, rtol=1e-5):
                print("âœ… æ··åˆæŸå¤±è®¡ç®—æ­£ç¡®")
                print(f"   æœŸæœ›: {expected_loss.item():.4f}")
                print(f"   å®é™…: {actual_loss.item():.4f}")
            else:
                print("âŒ æ··åˆæŸå¤±è®¡ç®—é”™è¯¯")
                print(f"   æœŸæœ›: {expected_loss.item():.4f}")
                print(f"   å®é™…: {actual_loss.item():.4f}")
                return False
        
        return True
        
    except Exception as e:
        print(f"âŒ æŸå¤±è®¡ç®—éªŒè¯å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("RL+BCæ··åˆè®­ç»ƒå®ç°éªŒè¯")
    print("="*60)
    
    results = []
    
    # 1. é…ç½®éªŒè¯
    results.append(("é…ç½®å‚æ•°", verify_config()))
    
    # 2. æ¨¡å‹è¾“å‡ºéªŒè¯
    results.append(("æ¨¡å‹è¾“å‡º", verify_model_output()))
    
    # 3. æŸå¤±è®¡ç®—éªŒè¯
    results.append(("æŸå¤±è®¡ç®—", verify_loss_computation()))
    
    # æ€»ç»“
    print("\n" + "="*60)
    print("éªŒè¯æ€»ç»“")
    print("="*60)
    
    all_passed = True
    for name, passed in results:
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"{name}: {status}")
        if not passed:
            all_passed = False
    
    print("\n" + "="*60)
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰éªŒè¯é€šè¿‡ï¼RL+BCå®ç°æ­£ç¡®ã€‚")
        print("\nå»ºè®®ä¸‹ä¸€æ­¥ï¼š")
        print("1. è¿è¡Œbaselineå®éªŒï¼ˆè®¾ç½® use_rl_bc_hybrid=Falseï¼‰")
        print("2. è¿è¡ŒRL+BCå®éªŒï¼ˆä¿å®ˆé…ç½®ï¼šlambda_bc=0.7ï¼‰")
        print("3. å¯¹æ¯”æ€§èƒ½æŒ‡æ ‡")
    else:
        print("âš ï¸  éƒ¨åˆ†éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°ã€‚")
    print("="*60)
    
    return 0 if all_passed else 1


if __name__ == "__main__":
    sys.exit(main())
