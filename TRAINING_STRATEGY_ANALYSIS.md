# è®­ç»ƒç­–ç•¥åˆ†æï¼šStage1 vs Stage2

## å½“å‰å®ç°ï¼šè”åˆè®­ç»ƒï¼ˆJoint Trainingï¼‰

### ä»£ç è¯æ®

```python
# pl_model.py line 248
loss = loss_action + loss_bis + loss_rp + loss_value + loss_reward
```

**æ‰€æœ‰ç»„ä»¶åŒæ—¶ä¼˜åŒ–**ï¼š
- `loss_bis`, `loss_rp`: ç”Ÿå‘½ä½“å¾é¢„æµ‹ï¼ˆç¯å¢ƒæ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼‰
- `loss_value`: ä»·å€¼å‡½æ•°ï¼ˆcriticï¼‰
- `loss_reward`: å¥–åŠ±é¢„æµ‹ï¼ˆç¯å¢ƒæ¨¡å‹ï¼‰
- `loss_action`: ç­–ç•¥å‡½æ•°ï¼ˆactorï¼ŒBC+MCTSï¼‰

### è®­ç»ƒæµç¨‹

```
æ¯ä¸ªè®­ç»ƒstepï¼š
1. Forward Stage1
   - ç”¨çœŸå®åŠ¨ä½œrollout
   - é¢„æµ‹ bis, rp, value, reward
   - è®¡ç®— loss_bis, loss_rp, loss_value, loss_reward

2. Forward Stage2
   - ç”¨é¢„æµ‹åŠ¨ä½œï¼ˆæˆ–MCTSåŠ¨ä½œï¼‰rollout
   - é¢„æµ‹ policy
   - è®¡ç®— loss_action (BC + MCTS)

3. ä¼˜åŒ–
   - loss_total = loss1 + loss2
   - ä¸€æ¬¡åå‘ä¼ æ’­æ›´æ–°æ‰€æœ‰å‚æ•°
```

---

## é—®é¢˜ï¼šç¯å¢ƒæ¨¡å‹èƒ½è®­ç»ƒå¥½å—ï¼Ÿ

### âš ï¸ æ½œåœ¨é—®é¢˜

#### é—®é¢˜1ï¼šStage2æ±¡æŸ“ç¯å¢ƒæ¨¡å‹

```python
# Stage2ä¸­ï¼ˆmodel.py line 265-398ï¼‰
for i in range(pre_len):
    policy_t, value_t = self.prediction(state_t)
    
    # ç”¨é¢„æµ‹åŠ¨ä½œrolloutï¼ˆä¸æ˜¯çœŸå®åŠ¨ä½œï¼ï¼‰
    if use_mcts:
        action = mcts.search(...)
    else:
        action = policy.argmax(...)
    
    # ç”¨é¢„æµ‹åŠ¨ä½œæ¨è¿›dynamics
    state_t_next = self.dynamics(state_t, action, ...)
    #                                     â†‘
    #                              ä¸æ˜¯çœŸå®åŠ¨ä½œï¼
```

**é—®é¢˜**ï¼š
- Stage2çš„dynamicsè°ƒç”¨æ—¶ï¼Œè¾“å…¥çš„æ˜¯**é¢„æµ‹åŠ¨ä½œ**
- dynamicsçš„æ¢¯åº¦ä¼šåå‘ä¼ æ’­
- dynamicså¯èƒ½å­¦åˆ°"å¯¹é¢„æµ‹åŠ¨ä½œæ›´å‹å¥½"è€ŒéçœŸå®çš„ç¯å¢ƒè½¬ç§»

**è¿™å«model exploitationï¼ˆæ¨¡å‹åˆ©ç”¨ï¼‰**ï¼š
```
dynamicsè¢«ä¼˜åŒ–æˆï¼š
  å¯¹policyé¢„æµ‹çš„åŠ¨ä½œ â†’ äº§ç”Ÿé«˜rewardçš„ä¸‹ä¸€çŠ¶æ€
è€Œä¸æ˜¯ï¼š
  çœŸå®åœ°æ¨¡æ‹Ÿç¯å¢ƒè½¬ç§»
```

#### é—®é¢˜2ï¼šè®­ç»ƒç›®æ ‡æ··æ‚

```python
# dynamicsåŒæ—¶æ¥æ”¶ä¸¤ç§è®­ç»ƒä¿¡å·

# Signal 1: Stage1çš„ç›‘ç£ä¿¡å·ï¼ˆå¥½çš„âœ…ï¼‰
state_next_çœŸå®, reward_çœŸå® = dynamics(state, action_çœŸå®, ...)
loss_dynamics = MSE(state_next_çœŸå®, state_target)

# Signal 2: Stage2çš„éšå¼ä¿¡å·ï¼ˆåçš„âŒï¼‰
state_next_é¢„æµ‹, reward_é¢„æµ‹ = dynamics(state, action_é¢„æµ‹, ...)
# è¿™ä¸ªè°ƒç”¨ä¹Ÿä¼šäº§ç”Ÿæ¢¯åº¦ï¼Œå› ä¸ºstate_nextå‚ä¸policy lossè®¡ç®—
loss_action = CE(policy, target)  # ä¾èµ–state_next
```

**å†²çª**ï¼š
- Stage1è¦æ±‚dynamicsçœŸå®æ¨¡æ‹Ÿç¯å¢ƒ
- Stage2éšå¼è¦æ±‚dynamicså¯¹policyå‹å¥½

#### é—®é¢˜3ï¼šè®­ç»ƒæ—©æœŸç¯å¢ƒæ¨¡å‹ä¸å‡†

```python
# è®­ç»ƒå¼€å§‹æ—¶
epoch 1-10: 
  - dynamics, reward, valueéƒ½ä¸å‡†
  - ä½†policyå·²ç»å¼€å§‹ç”¨å®ƒä»¬rollout
  - MCTSåœ¨é”™è¯¯çš„æ¨¡å‹ä¸Šæœç´¢
  - policyå­¦åˆ°é”™è¯¯çš„ç­–ç•¥
```

---

## è§£å†³æ–¹æ¡ˆ

### ğŸ¥‡ æ–¹æ¡ˆ1ï¼šä¸¤é˜¶æ®µè®­ç»ƒï¼ˆæ¨èï¼‰

#### Phase 1: åªè®­ç»ƒç¯å¢ƒæ¨¡å‹ï¼ˆwarm-upï¼‰

```python
# å‰Nä¸ªepochï¼ˆå¦‚N=10-20ï¼‰
if epoch < warmup_epochs:
    # åªä¼˜åŒ–ç¯å¢ƒæ¨¡å‹
    loss = loss_bis + loss_rp + loss_value + loss_reward
    # ä¸ä¼˜åŒ–policy
else:
    # æ­£å¸¸è”åˆè®­ç»ƒ
    loss = loss_action + loss_bis + loss_rp + loss_value + loss_reward
```

**ä¼˜ç‚¹**ï¼š
- âœ… ç¡®ä¿ç¯å¢ƒæ¨¡å‹å…ˆè®­ç»ƒå¥½
- âœ… MCTSåœ¨å‡†ç¡®çš„æ¨¡å‹ä¸Šæœç´¢
- âœ… policyä¸ä¼šå­¦åˆ°é”™è¯¯ä¿¡å·

**å®ç°**ï¼š
```python
# pl_model.py
class RLSLModelModule(BaseModule):
    def __init__(self, *args, warmup_epochs=10, **kwargs):
        super().__init__(*args, **kwargs)
        self.warmup_epochs = warmup_epochs
    
    def get_loss(self, pred_data, label_data):
        # ... è®¡ç®—æ‰€æœ‰loss ...
        
        # ä¸¤é˜¶æ®µç­–ç•¥
        if self.current_epoch < self.warmup_epochs:
            # Phase 1: åªè®­ç»ƒç¯å¢ƒæ¨¡å‹
            loss = loss_bis + loss_rp + loss_value + loss_reward
        else:
            # Phase 2: è”åˆè®­ç»ƒ
            loss = loss_action + loss_bis + loss_rp + loss_value + loss_reward
        
        return losses
```

### ğŸ¥ˆ æ–¹æ¡ˆ2ï¼šå†»ç»“dynamicsï¼ˆéƒ¨åˆ†ï¼‰

åœ¨Stage2ä¸­å†»ç»“dynamicsçš„æ¢¯åº¦ï¼š

```python
# model.py - Stage2
for i in range(pre_len):
    policy_t, value_t = self.prediction(state_t)
    
    # ç”¨é¢„æµ‹åŠ¨ä½œrolloutï¼Œä½†ä¸è®©dynamicsè¢«æ›´æ–°
    with torch.no_grad():
        # å†»ç»“dynamics
        state_t_next, reward_t = self.dynamics(state_t, action, ...)
    
    # æˆ–è€…detach state
    state_t = state_t_next.detach()
```

**ä¼˜ç‚¹**ï¼š
- âœ… dynamicsåªä»Stage1å­¦ä¹ ï¼ˆçœŸå®è½¬ç§»ï¼‰
- âœ… é¿å…model exploitation

**ç¼ºç‚¹**ï¼š
- âŒ dynamicså¯èƒ½ä¸é€‚åº”policyçš„åˆ†å¸ƒ

### ğŸ¥‰ æ–¹æ¡ˆ3ï¼šåˆ†ç¦»è®­ç»ƒï¼ˆæœ€æ¿€è¿›ï¼‰

å®Œå…¨åˆ†å¼€ä¸¤ä¸ªé˜¶æ®µï¼š

```python
# è®­ç»ƒå¾ªç¯
for epoch in range(n_epochs):
    # å…ˆè®­ç»ƒç¯å¢ƒæ¨¡å‹
    for batch in train_loader:
        loss_env = train_env_model(batch)
        optimizer_env.step()
    
    # å†è®­ç»ƒpolicy
    for batch in train_loader:
        loss_policy = train_policy(batch)
        optimizer_policy.step()
```

**ä¼˜ç‚¹**ï¼š
- âœ… å®Œå…¨è§£è€¦
- âœ… ç¯å¢ƒæ¨¡å‹ä¸å—policyå½±å“

**ç¼ºç‚¹**ï¼š
- âŒ è®­ç»ƒæ…¢ï¼ˆ2å€æ—¶é—´ï¼‰
- âŒ å®ç°å¤æ‚

### ğŸŒŸ æ–¹æ¡ˆ4ï¼šåŠ æƒæŸå¤±ï¼ˆç®€å•ï¼‰

è°ƒæ•´lossæƒé‡ï¼Œç¯å¢ƒæ¨¡å‹losså ä¸»å¯¼ï¼š

```python
# å½“å‰ï¼ˆé—®é¢˜ï¼‰
loss = loss_action + loss_bis + loss_rp + loss_value + loss_reward
#      æƒé‡1          æƒé‡1      æƒé‡1      æƒé‡1         æƒé‡1

# æ”¹è¿›
loss = 0.2 * loss_action + loss_bis + loss_rp + loss_value + loss_reward
#      â†“ é™ä½policyæƒé‡ï¼Œå‡å°‘å¯¹ç¯å¢ƒæ¨¡å‹çš„å¹²æ‰°
```

æˆ–è€…ï¼š

```python
# constant.py
env_loss_weight = 5.0
policy_loss_weight = 1.0

# pl_model.py
loss = (policy_loss_weight * loss_action + 
        env_loss_weight * (loss_bis + loss_rp + loss_value + loss_reward))
```

---

## æ¨èæ–¹æ¡ˆï¼ˆåŒ»ç–—åœºæ™¯ï¼‰

### ç»„åˆæ–¹æ¡ˆï¼šWarm-up + åŠ æƒ

```python
# constant.py
warmup_epochs = 10  # å‰10ä¸ªepochåªè®­ç»ƒç¯å¢ƒæ¨¡å‹
env_loss_weight = 2.0  # warmupåï¼Œç¯å¢ƒæ¨¡å‹lossæƒé‡ä»æ˜¯policyçš„2å€

# pl_model.py
if self.current_epoch < warmup_epochs:
    # Phase 1: åªè®­ç»ƒç¯å¢ƒæ¨¡å‹
    loss = loss_bis + loss_rp + loss_value + loss_reward
else:
    # Phase 2: è”åˆè®­ç»ƒï¼Œä½†ç¯å¢ƒæ¨¡å‹å ä¸»å¯¼
    loss_env = loss_bis + loss_rp + loss_value + loss_reward
    loss = loss_action + env_loss_weight * loss_env
```

**ç†ç”±**ï¼š
1. **Warm-upç¡®ä¿ç¯å¢ƒæ¨¡å‹å…ˆè®­ç»ƒå¥½**
   - å‰10 epochç¯å¢ƒæ¨¡å‹ä»éšæœºåˆå§‹åŒ–å­¦ä¹ 
   - dynamics, reward, value, bis, rpéƒ½å­¦åˆ°åˆç†çš„é¢„æµ‹

2. **åŠ æƒç¡®ä¿ç¯å¢ƒæ¨¡å‹æŒç»­å‡†ç¡®**
   - å³ä½¿è”åˆè®­ç»ƒï¼Œç¯å¢ƒæ¨¡å‹losså ä¸»å¯¼
   - å‡å°‘policyå¯¹ç¯å¢ƒæ¨¡å‹çš„å¹²æ‰°

3. **åŒ»ç–—åœºæ™¯é€‚ç”¨**
   - ç¯å¢ƒæ¨¡å‹å‡†ç¡®æ€§ä¼˜å…ˆï¼ˆæ‚£è€…å®‰å…¨ï¼‰
   - policyå¯ä»¥ç¨æ…¢æ”¶æ•›

---

## å½“å‰å®ç°èƒ½è®­ç»ƒå¥½å—ï¼Ÿ

### å›ç­”ä½ çš„é—®é¢˜

**èƒ½ï¼Œä½†ä¸ä¿è¯**ï¼š

âœ… **å¯èƒ½è®­ç»ƒå¥½çš„æƒ…å†µ**ï¼š
- æ•°æ®å……è¶³ä¸”è´¨é‡é«˜
- ç¯å¢ƒæ¨¡å‹æœ¬èº«å®¹æ˜“å­¦ï¼ˆdynamicsä¸å¤ªå¤æ‚ï¼‰
- å­¦ä¹ ç‡è®¾ç½®åˆç†
- è®­ç»ƒè¶³å¤Ÿé•¿æ—¶é—´

âŒ **å¯èƒ½è®­ç»ƒä¸å¥½çš„æƒ…å†µ**ï¼š
- è®­ç»ƒæ—©æœŸç¯å¢ƒæ¨¡å‹ä¸å‡†
- Policyåœ¨é”™è¯¯æ¨¡å‹ä¸Šå­¦ä¹ ï¼Œäº§ç”Ÿé”™è¯¯æ¢¯åº¦
- Model exploitationï¼šdynamicsè¿åˆpolicy
- MCTSåœ¨ä¸å‡†çš„æ¨¡å‹ä¸Šæœç´¢

### åˆ¤æ–­ç¯å¢ƒæ¨¡å‹æ˜¯å¦è®­ç»ƒå¥½

**ç›‘æ§æŒ‡æ ‡**ï¼š
```python
val_bis_mae      # BISé¢„æµ‹å‡†ç¡®æ€§ï¼ˆåº”è¯¥<3.0ï¼‰
val_rp_mae       # RPé¢„æµ‹å‡†ç¡®æ€§ï¼ˆåº”è¯¥<5.0ï¼‰
val_reward_mae   # Rewardé¢„æµ‹å‡†ç¡®æ€§
val_value_mae    # Valueé¢„æµ‹å‡†ç¡®æ€§

# å¥åº·çŠ¶æ€
- è¿™äº›æŒ‡æ ‡åº”è¯¥æŒç»­ä¸‹é™
- ä¸åº”è¯¥åœ¨æŸä¸ªepochåçªç„¶ä¸Šå‡
  ï¼ˆå¦‚æœä¸Šå‡ï¼Œè¯´æ˜policyå¼€å§‹å¹²æ‰°ç¯å¢ƒæ¨¡å‹ï¼‰
```

**è¯Šæ–­æ–¹æ³•**ï¼š
```python
# åœ¨validationæ—¶ï¼Œå•ç‹¬è¯„ä¼°ç¯å¢ƒæ¨¡å‹
with torch.no_grad():
    # ç”¨çœŸå®åŠ¨ä½œrolloutï¼ˆä¸ç”¨policyåŠ¨ä½œï¼‰
    for i in range(pre_len):
        state_next_çœŸå®, reward_çœŸå® = dynamics(state, action_çœŸå®[i], ...)
        bis_çœŸå® = output_bis(state_next_çœŸå®)
        
        # å’Œtargetæ¯”è¾ƒ
        env_model_error = MAE(bis_çœŸå®, bis_target[i])
    
    # å¦‚æœerrorå¾ˆå¤§ï¼Œè¯´æ˜ç¯å¢ƒæ¨¡å‹ä¸å‡†
```

---

## å®ç°å»ºè®®

### ç«‹å³å¯åšï¼ˆæœ€å°æ”¹åŠ¨ï¼‰

1. **æ·»åŠ warm-up**ï¼š
```python
# constant.py
warmup_epochs = 10

# pl_model.pyä¿®æ”¹get_loss
if self.current_epoch < warmup_epochs:
    loss = loss_bis + loss_rp + loss_value + loss_reward
else:
    loss = loss_action + loss_bis + loss_rp + loss_value + loss_reward
```

2. **ç›‘æ§ç¯å¢ƒæ¨¡å‹æŒ‡æ ‡**ï¼š
```python
# é‡ç‚¹å…³æ³¨
- loss_bisä¸‹é™æ›²çº¿
- loss_rpä¸‹é™æ›²çº¿
- loss_valueä¸‹é™æ›²çº¿
- loss_rewardä¸‹é™æ›²çº¿

# å¦‚æœè¿™äº›lossåœ¨warmupååå¼¹ï¼Œè¯´æ˜policyåœ¨å¹²æ‰°
```

### ä¸­æœŸæ”¹è¿›ï¼ˆæ¨èï¼‰

å®ç°æ–¹æ¡ˆ4ï¼ˆåŠ æƒæŸå¤±ï¼‰ï¼š
```python
# constant.py
warmup_epochs = 10
env_loss_weight = 2.0

# pl_model.py
if self.current_epoch < warmup_epochs:
    loss = loss_bis + loss_rp + loss_value + loss_reward
else:
    loss_env = loss_bis + loss_rp + loss_value + loss_reward
    loss = loss_action + env_loss_weight * loss_env
```

### é•¿æœŸä¼˜åŒ–ï¼ˆå¦‚æœæœ‰é—®é¢˜ï¼‰

è€ƒè™‘æ–¹æ¡ˆ2ï¼ˆå†»ç»“dynamicsï¼‰ï¼š
- åœ¨Stage2çš„forwardä¸­detach state
- ç¡®ä¿dynamicsåªä»Stage1å­¦ä¹ 

---

## æ€»ç»“

### å½“å‰å®ç°çš„é—®é¢˜

1. **è”åˆè®­ç»ƒ**ï¼šStage1å’ŒStage2åŒæ—¶ä¼˜åŒ–
2. **æ— ä¿è¯**ï¼šç¯å¢ƒæ¨¡å‹å¯èƒ½è¢«policyå¹²æ‰°
3. **é£é™©**ï¼šMCTSå¯èƒ½åœ¨ä¸å‡†çš„æ¨¡å‹ä¸Šæœç´¢

### æ¨èæ”¹è¿›

```python
# æœ€å°æ”¹åŠ¨ï¼Œæœ€å¤§æ”¶ç›Š
1. æ·»åŠ warm-upï¼ˆ10 epochsï¼‰
2. åŠ æƒæŸå¤±ï¼ˆenv_weight=2.0ï¼‰
3. ç›‘æ§ç¯å¢ƒæ¨¡å‹æŒ‡æ ‡
```

### åˆ¤æ–­æ ‡å‡†

**ç¯å¢ƒæ¨¡å‹è®­ç»ƒå¥½çš„æ ‡å¿—**ï¼š
- âœ… `loss_bis`, `loss_rp` æŒç»­ä¸‹é™ï¼Œä¸åå¼¹
- âœ… `val_bis_mae < 3.0`ï¼ˆå–å†³äºä½ çš„æ•°æ®ï¼‰
- âœ… `val_rp_mae < 5.0`
- âœ… åœ¨warmupåï¼Œè¿™äº›æŒ‡æ ‡ä»ç„¶ç¨³å®š

**ç¯å¢ƒæ¨¡å‹æœ‰é—®é¢˜çš„æ ‡å¿—**ï¼š
- âŒ warmupåï¼Œç¯å¢ƒlossåå¼¹
- âŒ val_bis_mae, val_rp_maeå¾ˆå¤§æˆ–ä¸æ”¶æ•›
- âŒ MCTSæ¿€æ´»ç‡æä½ï¼ˆ<1%ï¼‰æˆ–æé«˜ï¼ˆ>80%ï¼‰

å¸Œæœ›è¿™ä¸ªåˆ†ææœ‰å¸®åŠ©ï¼éœ€è¦æˆ‘å¸®ä½ å®ç°warm-upæœºåˆ¶å—ï¼Ÿ
