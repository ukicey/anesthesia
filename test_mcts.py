#!/usr/bin/env python3
"""
æµ‹è¯• MCTS æ¨¡å—æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import torch
import numpy as np
from RL.Module.model import TransformerPlanningModel
from constant import *

def test_mcts_basic():
    """æµ‹è¯• MCTS åŸºæœ¬åŠŸèƒ½"""
    print("=" * 60)
    print("æµ‹è¯• MCTS åŸºæœ¬åŠŸèƒ½")
    print("=" * 60)
    
    # åˆ›å»ºä¸€ä¸ªå°æ¨¡å‹ç”¨äºæµ‹è¯•
    model = TransformerPlanningModel(
        n_action=n_action,
        n_option=n_option,
        n_reward_max=n_reward_max,
        n_value_max=n_value_max,
        max_lenth=max_lenth,
        n_aux=3,
        n_input=100,  # ç®€åŒ–
        use_mcts=True,
        mcts_simulations=5  # å°‘é‡æ¨¡æ‹Ÿç”¨äºå¿«é€Ÿæµ‹è¯•
    )
    model.eval()
    
    # åˆ›å»ºéšæœºè¾“å…¥
    batch_size = 2
    obs = torch.randn(batch_size, max_lenth, 100)
    action_prev = (
        torch.randint(0, n_action, (batch_size, max_lenth)),
        torch.randint(0, n_action, (batch_size, max_lenth))
    )
    option = torch.randint(0, n_option, (batch_size, max_lenth))
    padding_mask = torch.zeros(batch_size, max_lenth, dtype=torch.bool)
    
    print(f"è¾“å…¥å½¢çŠ¶:")
    print(f"  obs: {obs.shape}")
    print(f"  action_prev: ({action_prev[0].shape}, {action_prev[1].shape})")
    print(f"  option: {option.shape}")
    
    # æµ‹è¯•åˆå§‹æ¨ç†
    print("\næµ‹è¯•åˆå§‹æ¨ç†...")
    with torch.no_grad():
        policy, state_0 = model.initial_inference(obs, action_prev, option, padding_mask)
    
    print(f"  state_0: {state_0.shape}")
    print(f"  policy_bbf: {policy[0].shape}, policy_rftn: {policy[1].shape}")
    
    # æµ‹è¯• MCTS æœç´¢
    print("\næµ‹è¯• MCTS æœç´¢...")
    if model.use_mcts:
        state_t = state_0
        action_mcts = model.mcts.search_sequential(
            state_0[0:1], state_t[0:1], 
            (action_prev[0][0:1], action_prev[1][0:1]),
            option[0:1], padding_mask=padding_mask[0:1], offset=0
        )
        print(f"  MCTS é€‰æ‹©çš„åŠ¨ä½œ: BBF={action_mcts[0]}, RFTN={action_mcts[1]}")
        print(f"  âœ… MCTS æœç´¢æˆåŠŸï¼")
    else:
        print("  âŒ MCTS æœªå¯ç”¨")
    
    # æµ‹è¯•è´ªå©ªé€‰æ‹©ï¼ˆå¯¹æ¯”ï¼‰
    print("\næµ‹è¯•è´ªå©ªé€‰æ‹©ï¼ˆå¯¹æ¯”ï¼‰...")
    greedy_action = (
        policy[0][0].argmax().item(),
        policy[1][0].argmax().item()
    )
    print(f"  è´ªå©ªé€‰æ‹©çš„åŠ¨ä½œ: BBF={greedy_action[0]}, RFTN={greedy_action[1]}")
    
    print("\n" + "=" * 60)
    print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    print("=" * 60)


def test_mcts_forward():
    """æµ‹è¯•å®Œæ•´çš„ forward è¿‡ç¨‹"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•å®Œæ•´ forward è¿‡ç¨‹ï¼ˆå¸¦ MCTSï¼‰")
    print("=" * 60)
    
    model = TransformerPlanningModel(
        n_action=n_action,
        n_option=n_option,
        n_reward_max=n_reward_max,
        n_value_max=n_value_max,
        max_lenth=max_lenth,
        n_aux=3,
        n_input=100,
        use_mcts=True,
        mcts_simulations=3  # æå°‘æ¨¡æ‹Ÿï¼Œåªæµ‹è¯•æµç¨‹
    )
    model.eval()
    
    batch_size = 1
    total_len = max_lenth + pre_len
    
    obs = torch.randn(batch_size, max_lenth, 100)
    action_prev = (
        torch.randint(0, n_action, (batch_size, total_len)),
        torch.randint(0, n_action, (batch_size, total_len))
    )
    option = torch.randint(0, n_option, (batch_size, total_len))
    padding_mask = torch.zeros(batch_size, max_lenth, dtype=torch.bool)
    
    print("æ‰§è¡Œ forwardï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰...")
    
    try:
        with torch.no_grad():
            outputs = model(obs, action_prev, option, padding_mask, gamma=0.9)
        
        print(f"\nè¾“å‡ºå½¢çŠ¶:")
        print(f"  policy_bbf: {outputs['policy'][0].shape}")
        print(f"  policy_rftn: {outputs['policy'][1].shape}")
        print(f"  value: {outputs['value'].shape}")
        print(f"  reward: {outputs['reward'].shape}")
        print(f"  bis: {outputs['bis'].shape}")
        print(f"  rp: {outputs['rp'].shape}")
        
        print("\nâœ… Forward æµ‹è¯•é€šè¿‡ï¼MCTS åœ¨ Stage 2 æ­£å¸¸å·¥ä½œ")
        
    except Exception as e:
        print(f"\nâŒ Forward æµ‹è¯•å¤±è´¥: {e}")
        raise
    
    print("=" * 60)


def compare_with_without_mcts():
    """å¯¹æ¯”æœ‰æ—  MCTS çš„å·®å¼‚"""
    print("\n" + "=" * 60)
    print("å¯¹æ¯”æœ‰æ—  MCTS çš„å·®å¼‚")
    print("=" * 60)
    
    # å‡†å¤‡ç›¸åŒçš„è¾“å…¥
    batch_size = 1
    total_len = max_lenth + pre_len
    
    torch.manual_seed(42)  # å›ºå®šéšæœºç§å­
    obs = torch.randn(batch_size, max_lenth, 100)
    action_prev = (
        torch.randint(0, n_action, (batch_size, total_len)),
        torch.randint(0, n_action, (batch_size, total_len))
    )
    option = torch.randint(0, n_option, (batch_size, total_len))
    padding_mask = torch.zeros(batch_size, max_lenth, dtype=torch.bool)
    
    # æ¨¡å‹ 1: ä¸ä½¿ç”¨ MCTS
    print("\n1ï¸âƒ£ ä¸ä½¿ç”¨ MCTSï¼ˆè´ªå©ªé€‰æ‹©ï¼‰")
    model_no_mcts = TransformerPlanningModel(
        n_action=n_action, n_option=n_option, n_reward_max=n_reward_max,
        n_value_max=n_value_max, max_lenth=max_lenth, n_aux=3, n_input=100,
        use_mcts=False
    )
    model_no_mcts.eval()
    
    import time
    start = time.time()
    with torch.no_grad():
        outputs_no_mcts = model_no_mcts(obs, action_prev, option, padding_mask)
    time_no_mcts = time.time() - start
    
    print(f"  æ‰§è¡Œæ—¶é—´: {time_no_mcts:.3f}s")
    print(f"  å¹³å‡å¥–åŠ±: {outputs_no_mcts['reward'].mean().item():.4f}")
    
    # æ¨¡å‹ 2: ä½¿ç”¨ MCTS
    print("\n2ï¸âƒ£ ä½¿ç”¨ MCTSï¼ˆæ ‘æœç´¢ï¼‰")
    model_with_mcts = TransformerPlanningModel(
        n_action=n_action, n_option=n_option, n_reward_max=n_reward_max,
        n_value_max=n_value_max, max_lenth=max_lenth, n_aux=3, n_input=100,
        use_mcts=True, mcts_simulations=5
    )
    
    # å¤åˆ¶æƒé‡ç¡®ä¿å…¬å¹³å¯¹æ¯”
    model_with_mcts.load_state_dict(model_no_mcts.state_dict(), strict=False)
    model_with_mcts.eval()
    
    start = time.time()
    with torch.no_grad():
        outputs_with_mcts = model_with_mcts(obs, action_prev, option, padding_mask)
    time_with_mcts = time.time() - start
    
    print(f"  æ‰§è¡Œæ—¶é—´: {time_with_mcts:.3f}s")
    print(f"  å¹³å‡å¥–åŠ±: {outputs_with_mcts['reward'].mean().item():.4f}")
    
    # å¯¹æ¯”
    print("\nğŸ“Š å¯¹æ¯”ç»“æœ:")
    print(f"  æ—¶é—´å¼€é”€: {time_with_mcts / time_no_mcts:.2f}x")
    print(f"  å¥–åŠ±å·®å¼‚: {outputs_with_mcts['reward'].mean().item() - outputs_no_mcts['reward'].mean().item():.4f}")
    
    print("\nğŸ’¡ è¯´æ˜:")
    print("  - MCTS ä¼šå¢åŠ è®¡ç®—æ—¶é—´ï¼ˆæ­£å¸¸ç°è±¡ï¼‰")
    print("  - å¥–åŠ±å·®å¼‚å¯èƒ½å¾ˆå°ï¼ˆå› ä¸ºæ¨¡å‹æœªè®­ç»ƒï¼‰")
    print("  - è®­ç»ƒåï¼ŒMCTS é€šå¸¸èƒ½è·å¾—æ›´é«˜å¥–åŠ±")
    
    print("=" * 60)


if __name__ == '__main__':
    print("\nğŸ§ª MCTS æ¨¡å—æµ‹è¯•\n")
    
    try:
        # æµ‹è¯• 1: åŸºæœ¬åŠŸèƒ½
        test_mcts_basic()
        
        # æµ‹è¯• 2: Forward è¿‡ç¨‹
        test_mcts_forward()
        
        # æµ‹è¯• 3: å¯¹æ¯”å®éªŒ
        compare_with_without_mcts()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼MCTS æ¨¡å—å·¥ä½œæ­£å¸¸")
        print("=" * 60)
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print("  1. åœ¨ constant.py ä¸­è®¾ç½® use_mcts=True")
        print("  2. è¿è¡Œ python train.py train å¼€å§‹è®­ç»ƒ")
        print("  3. ç›‘æ§è®­ç»ƒæŒ‡æ ‡ï¼ˆlossã€MAEï¼‰çš„æ”¹å–„")
        print("  4. è°ƒæ•´ mcts_simulations å¹³è¡¡é€Ÿåº¦å’Œæ€§èƒ½")
        print("=" * 60 + "\n")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        print("\nè¯·æ£€æŸ¥:")
        print("  1. æ˜¯å¦æ­£ç¡®å®‰è£…äº†æ‰€æœ‰ä¾èµ–")
        print("  2. constant.py ä¸­çš„å‚æ•°æ˜¯å¦æ­£ç¡®")
        print("  3. MCTS æ¨¡å—æ˜¯å¦æ­£ç¡®å¯¼å…¥")
